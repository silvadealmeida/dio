{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnCM5XnwDMw9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "#if using Theano with GPU\n",
        "#os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.models import Model\n",
        "\n"
      ],
      "metadata": {
        "id": "-prC1Oj3EP3-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "f2d0e19b-5acc-4d51-b5d1-c7f835d67a3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Backend 'tpu' failed to initialize: INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory. Available backends are ['cpu']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fcfab5ec33bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdevices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mdata_parallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36mdevices\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDevice\u001b[0m \u001b[0msubclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m   \"\"\"\n\u001b[0;32m-> 1083\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36mget_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0mplatform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mxla_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m ) -> xla_client.Client:\n\u001b[0;32m-> 1017\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_get_backend_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36m_get_backend_uncached\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mplatform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_backend_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m         raise RuntimeError(f\"Backend '{platform}' failed to initialize: \"\n\u001b[0m\u001b[1;32m   1003\u001b[0m                            \u001b[0;34mf\"{_backend_errors[platform]}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                            f'Available backends are {list(bs)}')\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Backend 'tpu' failed to initialize: INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory. Available backends are ['cpu']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Alternativa com TPU\n",
        "# import os\n",
        "# os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "# !pip install tensorflow-cpu\n",
        "# import tensorflow as tf\n",
        "# import keras"
      ],
      "metadata": {
        "id": "fponnQZFfp-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O primeiro passo será carregar nossos dados. Como nosso exemplo, usaremos o conjunto de dados CalTech-101 , que contém cerca de 9000 imagens rotuladas pertencentes a 101 categorias de objetos. No entanto, excluiremos 5 das categorias que têm mais imagens. Isso é para manter a distribuição de classes razoavelmente equilibrada (cerca de 50-100) e restrita a um número menor de imagens, cerca de 6000.\n",
        "\n",
        "Para obter esse conjunto de dados, você pode executar o script de download download.shna datapasta ou os seguintes comandos:\n",
        "\n",
        "    wget http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\n",
        "    tar -xvzf 101_ObjectCategories.tar.gz\n",
        "\n",
        "Se você deseja usar seu próprio conjunto de dados, ele deve ser organizado da mesma forma com 101_ObjectCategoriestodas as imagens organizadas em subpastas, uma para cada classe. Neste caso, a célula a seguir deve carregar seu conjunto de dados personalizado corretamente, apenas substituindo-o rootpela sua pasta. Se você tiver uma estrutura alternativa, você só precisa ter certeza de carregar a lista dataonde cada elemento é um dict onde xsão os dados (uma matriz numpy 1-d) e yé o rótulo (um inteiro). Use a função auxiliar get_image(path)para carregar a imagem corretamente na matriz e observe também que as imagens estão sendo redimensionadas para 224x224. Isso é necessário porque a entrada para VGG16 é uma imagem RGB de 224x224. Você não precisa redimensioná-las no seu disco rígido, pois isso está sendo feito no código abaixo.\n",
        "\n",
        "Se você tiver 101_ObjectCategoriesem sua pasta de dados, a célula a seguir deverá carregar todos os dados."
      ],
      "metadata": {
        "id": "ObTWhYO4F7S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!echo \"Downloading 101_Object_Categories for image notebooks\"\n",
        "!curl -L -o caltech-101.zip --progress-bar https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip?download=1 > caltech-101.zip\n",
        "!file caltech-101.zip\n",
        "!unzip -o caltech-101.zip\n",
        "!rm caltech-101.zip\n",
        "!cp caltech-101/101_ObjectCategories.tar.gz 101_ObjectCategories.tar.gz\n",
        "!rm -rf caltech-101/\n",
        "\n",
        "!tar -xzf 101_ObjectCategories.tar.gz\n",
        "!rm 101_ObjectCategories.tar.gz\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "mgzwMI6_EP7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta função é útil para pré-processar os dados em uma imagem e um vetor de entrada."
      ],
      "metadata": {
        "id": "KQBvWKj1GdhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = '101_ObjectCategories'\n",
        "exclude = ['BACKGROUND_Google', 'Motorbikes', 'airplanes', 'Faces_easy', 'Faces']\n",
        "train_split, val_split = 0.7, 0.15\n",
        "\n",
        "categories = [x[0] for x in os.walk(root) if x[0]][1:]\n",
        "categories = [c for c in categories if c not in [os.path.join(root, e) for e in exclude]]\n",
        "\n",
        "print(categories)"
      ],
      "metadata": {
        "id": "tOuflTLmJMS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to load image and return it and input vector\n",
        "def get_image(path):\n",
        "    img = image.load_img(path, target_size=(224, 224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return img, x"
      ],
      "metadata": {
        "id": "hA3ISJg-Ggws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregue todas as imagens da pasta raiz"
      ],
      "metadata": {
        "id": "5TNqBBFrGkf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for c, category in enumerate(categories):\n",
        "    images = [os.path.join(dp, f) for dp, dn, filenames\n",
        "              in os.walk(category) for f in filenames\n",
        "              if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
        "    for img_path in images:\n",
        "        img, x = get_image(img_path)\n",
        "        data.append({'x':np.array(x[0]), 'y':c})\n",
        "\n",
        "# count the number of classes\n",
        "num_classes = len(categories)"
      ],
      "metadata": {
        "id": "QDefcB8oEP-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomize a ordem dos dados."
      ],
      "metadata": {
        "id": "tJHi0iCkGpuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(data)"
      ],
      "metadata": {
        "id": "bVEHyOw5GqS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "criar divisão de treinamento/validação/teste (70%, 15%, 15%)"
      ],
      "metadata": {
        "id": "LJiZBUOTGszG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_val = int(train_split * len(data))\n",
        "idx_test = int((train_split + val_split) * len(data))\n",
        "train = data[:idx_val]\n",
        "val = data[idx_val:idx_test]\n",
        "test = data[idx_test:]"
      ],
      "metadata": {
        "id": "mvctz9GjG0ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dados separados para rótulos."
      ],
      "metadata": {
        "id": "D4A6kXRoGyrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = np.array([t[\"x\"] for t in train]), [t[\"y\"] for t in train]\n",
        "x_val, y_val = np.array([t[\"x\"] for t in val]), [t[\"y\"] for t in val]\n",
        "x_test, y_test = np.array([t[\"x\"] for t in test]), [t[\"y\"] for t in test]\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "yxnv_54REQIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pré-processe os dados como antes, certificando-se de que sejam float32 e normalizados entre 0 e 1."
      ],
      "metadata": {
        "id": "LcptKk3VHHVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize data\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_val = x_val.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# convert labels to one-hot vectors\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "8iBwWoPrHLHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos fazer um resumo do que temos."
      ],
      "metadata": {
        "id": "PiCyfNxkHVaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summary\n",
        "print(\"finished loading %d images from %d categories\"%(len(data), num_classes))\n",
        "print(\"train / validation / test split: %d, %d, %d\"%(len(x_train), len(x_val), len(x_test)))\n",
        "print(\"training data shape: \", x_train.shape)\n",
        "print(\"training labels shape: \", y_train.shape)\n"
      ],
      "metadata": {
        "id": "wPGGob2eHWRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se tudo funcionou corretamente, você deve ter carregado um monte de imagens e dividido elas em três conjuntos: `train`, `val`, e `test`. O formato dos dados de treinamento deve ser ( `n`, 224, 224, 3) onde né o tamanho do seu conjunto de treinamento, e os rótulos devem ser ( `n`, `c`) onde cé o número de classes (97 no caso de `101_ObjectCategories`).\n",
        "\n",
        "Observe que dividimos todos os dados em três subconjuntos — um conjunto de treinamento train, um conjunto de validação vale um conjunto de teste test. O motivo para isso é avaliar adequadamente a precisão do nosso classificador. Durante o treinamento, o otimizador usa o conjunto de validação para avaliar seu desempenho interno, a fim de determinar o gradiente sem overfitting ao conjunto de treinamento. O testconjunto é sempre mantido fora do algoritmo de treinamento e é usado apenas no final para avaliar a precisão final do nosso modelo.\n",
        "\n",
        "Vamos dar uma olhada rápida em algumas imagens de exemplo do nosso conjunto de dados."
      ],
      "metadata": {
        "id": "1Th1fz1PHeDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
        "idx = [int(len(images) * random.random()) for i in range(8)]\n",
        "imgs = [image.load_img(images[i], target_size=(224, 224)) for i in idx]\n",
        "concat_image = np.concatenate([np.asarray(img) for img in imgs], axis=1)\n",
        "plt.figure(figsize=(16,4))\n",
        "plt.imshow(concat_image)"
      ],
      "metadata": {
        "id": "OzITLWJRHfRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando Rede VGG16 do Keras\n",
        "treinada par ao ImageNet"
      ],
      "metadata": {
        "id": "VeZ_M1soJiKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "estratégia principal para treinar um classificador de imagens em nosso pequeno conjunto de dados: começar com uma rede maior e já treinada.\n",
        "Carregando Rede VGG16 do Keras\n",
        "treinada par ao ImageNet\n",
        "Assim que a rede for carregada, podemos inspecionar novamente as camadas com o summary()método."
      ],
      "metadata": {
        "id": "Oe22EsuoDpUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = keras.applications.VGG16(weights='imagenet', include_top=True)\n",
        "vgg.summary()"
      ],
      "metadata": {
        "id": "-m-P7eDQDPln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " VGG16 contém 13 camadas convolucionais e duas camadas totalmente conectadas no final, e tem mais de 138 milhões de parâmetros, cerca de 100 vezes mais parâmetros do que a rede que fizemos acima. Como nossa primeira rede, a maioria dos parâmetros é armazenada nas conexões que levam à primeira camada totalmente conectada.\n",
        "\n",
        " O VGG16 foi criado para resolver o ImageNet e atinge uma taxa de erro top-5 de 8,8% , o que significa que 91,2% das amostras de teste foram classificadas corretamente dentro das 5 principais previsões para cada imagem. Sua precisão top-1 — equivalente à métrica de precisão que temos usado (que a previsão top está correta) — é de 73%. Isso é especialmente impressionante, pois não há apenas 97, mas 1000 classes, o que significa que palpites aleatórios nos dariam apenas 0,1% de precisão.\n",
        "\n",
        "Para usar essa rede em nossa tarefa, \"removemos\" a camada de classificação final, a camada softmax de 1000 neurônios no final, que corresponde ao ImageNet, e a substituímos por uma nova camada softmax para nosso conjunto de dados, que contém 97 neurônios no caso do conjunto de dados 101_ObjectCategories.\n",
        "\n",
        "Em termos de implementação, é mais fácil simplesmente criar uma cópia do VGG de sua camada de entrada até a segunda camada até a última, e então trabalhar com isso, em vez de modificar o objeto VGG diretamente. Então, tecnicamente, nunca \"removemos\" nada, apenas contornamos/ignoramos. Isso pode ser feito da seguinte maneira, usando a Modelclasse keras para inicializar um novo modelo cuja camada de entrada é a mesma que VGG, mas cuja camada de saída é nossa nova camada softmax, chamada new_classification_layer. Nota: embora pareça que estamos duplicando essa grande rede, internamente o Keras está na verdade apenas copiando todas as camadas por referência, e assim não precisamos nos preocupar em sobrecarregar a memória."
      ],
      "metadata": {
        "id": "3Ux5zXH-Ewjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a reference to VGG's input layer\n",
        "inp = vgg.input\n",
        "\n",
        "# make a new softmax layer with num_classes neurons\n",
        "new_classification_layer = Dense(num_classes, activation='softmax')\n",
        "\n",
        "# connect our new layer to the second to last layer in VGG, and make a reference to it\n",
        "out = new_classification_layer(vgg.layers[-2].output)\n",
        "\n",
        "# create a new network between inp and out\n",
        "model_new = Model(inp, out)\n"
      ],
      "metadata": {
        "id": "JiZaPxgfExT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos treinar novamente essa rede, `model_new`no novo conjunto de dados e rótulos. Mas, primeiro, precisamos congelar os pesos e vieses em todas as camadas da rede, exceto a nossa nova no final, com a expectativa de que os recursos que foram aprendidos no VGG ainda sejam razoavelmente relevantes para a nova tarefa de classificação de imagens. Não é o ideal, mas provavelmente é melhor do que podemos treinar em nosso conjunto de dados limitado.\n",
        "\n",
        "Ao definir o trainablesinalizador em cada camada como falso (exceto nossa nova camada de classificação), garantimos que todos os pesos e vieses nessas camadas permaneçam fixos e simplesmente treinamos os pesos em uma camada no final. Em alguns casos, é desejável não congelar todas as camadas de pré-classificação. Se seu conjunto de dados tiver amostras suficientes e não se assemelhar muito ao ImageNet, pode ser vantajoso ajustar algumas das camadas VGG junto com o novo classificador, ou possivelmente até mesmo todas elas. Para fazer isso, você pode alterar o código abaixo para tornar mais camadas treináveis.\n",
        "\n",
        "No caso do CalTech-101, faremos apenas a extração de características, temendo que o ajuste fino demais com esse conjunto de dados possa causar overfit. Mas talvez estejamos errados? Um bom exercício seria experimentar ambos e comparar os resultados.\n",
        "\n",
        "Então, vamos em frente e congelamos as camadas, e compilamos o novo modelo com exatamente o mesmo otimizador e função de perda da nossa primeira rede, para fins de comparação justa. Então, rodamos summarynovamente para olhar a arquitetura da rede."
      ],
      "metadata": {
        "id": "0pGF3gTkFelE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make all layers untrainable by freezing weights (except for last layer)\n",
        "for l, layer in enumerate(model_new.layers[:-1]):\n",
        "    layer.trainable = False\n",
        "\n",
        "# ensure the last layer is trainable/not frozen\n",
        "for l, layer in enumerate(model_new.layers[-1:]):\n",
        "    layer.trainable = True\n",
        "\n",
        "model_new.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_new.summary()"
      ],
      "metadata": {
        "id": "ywI1gTcyFmgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Olhando para o resumo, vemos que a rede é idêntica ao modelo VGG que instanciamos anteriormente, exceto que a última camada, anteriormente um softmax de 1000 neurônios, foi substituída por um novo softmax de 97 neurônios. Além disso, ainda temos aproximadamente 134 milhões de pesos, mas agora a grande maioria deles são \"parâmetros não treináveis\" porque congelamos as camadas em que estão contidos. Agora temos apenas 397.000 parâmetros treináveis, o que na verdade é apenas um quarto do número de parâmetros necessários para treinar o primeiro modelo.\n",
        "\n",
        "Como antes, seguimos em frente e treinamos o novo modelo, usando os mesmos hiperparâmetros (tamanho do lote e número de épocas) de antes, junto com o mesmo algoritmo de otimização. Também mantemos o controle do histórico dele conforme avançamos."
      ],
      "metadata": {
        "id": "npiPqSWnFjOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = model_new.fit(x_train, y_train,\n",
        "                         batch_size=128,\n",
        "                         epochs=10,\n",
        "                         validation_data=(x_val, y_val))\n"
      ],
      "metadata": {
        "id": "yVrjqZBDJ8yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nossa precisão de validação fica próxima de 80% no final, o que representa uma melhoria de mais de 30% na rede original treinada do zero (o que significa que fazemos a previsão errada em 20% das amostras, em vez de 50%).\n",
        "\n",
        "Vale a pena notar também que essa rede na verdade treina um pouco mais rápido que a rede original, apesar de ter mais de 100 vezes mais parâmetros! Isso ocorre porque congelar os pesos nega a necessidade de retropropagar por todas essas camadas, economizando tempo de execução.\n",
        "\n",
        "Vamos traçar novamente a perda de validação e a precisão, desta vez comparando o modelo original treinado do zero (em azul) e o novo modelo aprendido por transferência em verde."
      ],
      "metadata": {
        "id": "nCVHYGChKCMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(16,4))\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(history.history[\"val_loss\"])\n",
        "ax.plot(history2.history[\"val_loss\"])\n",
        "ax.set_title(\"validation loss\")\n",
        "ax.set_xlabel(\"epochs\")\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.plot(history.history[\"val_acc\"])\n",
        "ax2.plot(history2.history[\"val_acc\"])\n",
        "ax2.set_title(\"validation accuracy\")\n",
        "ax2.set_xlabel(\"epochs\")\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zi8Bi7GjKCs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe que, enquanto o modelo original começou a sobreajustar em torno da época 16, o novo modelo continuou a diminuir lentamente sua perda ao longo do tempo e provavelmente teria melhorado sua precisão ligeiramente com mais iterações. O novo modelo chegou a aproximadamente 80% de precisão top-1 (no conjunto de validação) e continuou a melhorar lentamente ao longo de 100 épocas.\n",
        "\n",
        "É possível que pudéssemos ter melhorado o modelo original com melhor regularização ou mais abandono, mas certamente não teríamos compensado a melhoria de >30% na precisão.\n",
        "\n",
        "Novamente, fazemos uma validação final no conjunto de teste."
      ],
      "metadata": {
        "id": "bQZzza7AKIfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "mkbn7JJ7KKzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para prever uma nova imagem, basta executar o código a seguir para obter as probabilidades de cada classe."
      ],
      "metadata": {
        "id": "x-kNubu1KLOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img, x = get_image('101_ObjectCategories/airplanes/image_0003.jpg')\n",
        "probabilities = model_new.predict([x])"
      ],
      "metadata": {
        "id": "xUsxa5rCKQ8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Melhorando os resultados\n",
        "78,2% de precisão top-1 em 97 classes, distribuídas de forma aproximadamente uniforme, é uma conquista muito boa. Não é tão impressionante quanto o VGG16 original, que atingiu 73% de precisão top-1 em 1000 classes. No entanto, é muito melhor do que o que conseguimos atingir com nossa rede original, e há espaço para melhorias. Algumas técnicas que possivelmente poderiam ter melhorado nosso desempenho.\n",
        "\n",
        "- Uso de aumento de dados: aumento se refere ao uso de várias modificações dos dados de treinamento originais, na forma de distorções, rotações, redimensionamentos, alterações de iluminação, etc. para aumentar o tamanho do conjunto de treinamento e criar mais tolerância para tais distorções.\n",
        "- Usando um otimizador diferente, adicionando mais regularização/eliminação e outros hiperparâmetros.\n",
        "Treinar por mais tempo (claro)\n",
        "\n",
        "Um exemplo mais avançado de aprendizagem de transferência no Keras, envolvendo aumento para um pequeno conjunto de dados de 2 classes, pode ser encontrado no blog do Keras [Keras blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)."
      ],
      "metadata": {
        "id": "gq1oqF4nKbLl"
      }
    }
  ]
}